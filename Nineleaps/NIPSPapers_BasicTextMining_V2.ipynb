{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Approach 1"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas\n# load the dataset\ndataset = pandas.read_csv('/kaggle/input/nips-papers/papers.csv')\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset[dataset['abstract']!='Abstract Missing']\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fetch word count for each abstract"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fetch wordcount for each abstract\ndataset['word_count'] = dataset['abstract'].apply(lambda x: len(str(x).split(\" \")))\ndataset[['abstract','word_count']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Descriptive statistics of word counts\ndataset.word_count.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Most common and uncommon words"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Identify common words\nfreq = pandas.Series(' '.join(dataset['abstract']).split()).value_counts()[:20]\nfreq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Identify uncommon words\nfreq1 =  pandas.Series(' '.join(dataset \n         ['abstract']).split()).value_counts()[-20:]\nfreq1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text Pre-processing"},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/1627/1*yOna3oFFjHmCRti1qusMAA.png)"},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/1570/1*vwT-_4Eo5vcNrQ9JNl1gYg.png)"},{"metadata":{},"cell_type":"markdown","source":"**Stemming normalizes text by removing suffixes.**\n\n**Lemmatisation is a more advanced technique which works based on the root of the word.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem.porter import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nlem = WordNetLemmatizer()\nstem = PorterStemmer()\nword = \"inversely\"\nprint(\"stemming:\",stem.stem(word))\nprint(\"lemmatization:\", lem.lemmatize(word, \"v\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Libraries for text preprocessing\nimport re\nimport nltk\n#nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import RegexpTokenizer\n#nltk.download('wordnet') \nfrom nltk.stem.wordnet import WordNetLemmatizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Removing stopwords**"},{"metadata":{"trusted":true},"cell_type":"code","source":"##Creating a list of stop words and adding custom stopwords\nstop_words = set(stopwords.words(\"english\"))\n##Creating a list of custom stopwords\nnew_words = [\"using\", \"show\", \"result\", \"large\", \"also\", \"iv\", \"one\", \"two\", \"new\", \"previously\", \"shown\"]\nstop_words = stop_words.union(new_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['abstract'][100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = []\nfor i in range(0, 3847):\n    #Remove punctuations\n    text = re.sub('[^a-zA-Z]', ' ', dataset['abstract'][i])\n    \n    #Convert to lowercase\n    text = text.lower()\n    \n    #remove tags\n    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n    \n    # remove special characters and digits\n    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n    \n    ##Convert to list from string\n    text = text.split()\n    \n    ##Stemming\n    ps=PorterStemmer()\n    #Lemmatisation\n    lem = WordNetLemmatizer()\n    text = [lem.lemmatize(word) for word in text if not word in  \n            stop_words] \n    text = \" \".join(text)\n    corpus.append(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#View corpus item\ncorpus[100]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Word cloud\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\n%matplotlib inline\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stop_words,\n                          max_words=100,\n                          max_font_size=50, \n                          random_state=42\n                         ).generate(str(corpus))\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()\nfig.savefig(\"word1.png\", dpi=900)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text preparation\n*Tokenisation is the process of converting the continuous text into a list of words. The list of words is then converted to a matrix of integers by the process of vectorisation. Vectorisation is also called feature extraction.*"},{"metadata":{},"cell_type":"markdown","source":"## Creating a vector of word counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nimport re\ncv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,3))\nX=cv.fit_transform(corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(cv.vocabulary_.keys())[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## n-grams\n**Visualize top N uni-grams, bi-grams & tri-grams**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Most frequently occuring words\ndef get_top_n_words(corpus, n=None):\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n                   vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                       reverse=True)\n    return words_freq[:n]\n\n#Convert most freq words to dataframe for plotting bar plot\ntop_words = get_top_n_words(corpus, n=20)\ntop_df = pandas.DataFrame(top_words)\ntop_df.columns=[\"Word\", \"Freq\"]\n\n#Barplot of most freq words\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\ng = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df)\ng.set_xticklabels(g.get_xticklabels(), rotation=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Most frequently occuring Bi-grams\ndef get_top_n2_words(corpus, n=None):\n    vec1 = CountVectorizer(ngram_range=(2,2),  \n            max_features=2000).fit(corpus)\n    bag_of_words = vec1.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n                  vec1.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                reverse=True)\n    return words_freq[:n]\ntop2_words = get_top_n2_words(corpus, n=20)\ntop2_df = pandas.DataFrame(top2_words)\ntop2_df.columns=[\"Bi-gram\", \"Freq\"]\nprint(top2_df)\n#Barplot of most freq Bi-grams\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\nh=sns.barplot(x=\"Bi-gram\", y=\"Freq\", data=top2_df)\nh.set_xticklabels(h.get_xticklabels(), rotation=45)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Most frequently occuring Tri-grams\ndef get_top_n3_words(corpus, n=None):\n    vec1 = CountVectorizer(ngram_range=(3,3), \n           max_features=2000).fit(corpus)\n    bag_of_words = vec1.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n                  vec1.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                reverse=True)\n    return words_freq[:n]\ntop3_words = get_top_n3_words(corpus, n=20)\ntop3_df = pandas.DataFrame(top3_words)\ntop3_df.columns=[\"Tri-gram\", \"Freq\"]\nprint(top3_df)\n#Barplot of most freq Tri-grams\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\nj=sns.barplot(x=\"Tri-gram\", y=\"Freq\", data=top3_df)\nj.set_xticklabels(j.get_xticklabels(), rotation=45)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Converting to a matrix of integers\n## TF-IDF vectoriser"},{"metadata":{},"cell_type":"markdown","source":"**TF-IDF consists of 2 components:**\n* TF — term frequency\n* IDF — Inverse document frequency\n\n![](https://miro.medium.com/max/651/1*H0aRm_BHC_3QZIopp1iMcQ.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\n \ntfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\ntfidf_transformer.fit(X)\n# get feature names\nfeature_names=cv.get_feature_names()\n \n# fetch document for which keywords needs to be extracted\ndoc=corpus[532]\n \n#generate tf-idf for the given document\ntf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Based on the TF-IDF scores, we can extract the words with the highest scores to get the keywords for a document.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function for sorting tf_idf in descending order\nfrom scipy.sparse import coo_matrix\ndef sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n \ndef extract_topn_from_vector(feature_names, sorted_items, topn=10):\n    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n    \n    #use only topn items from vector\n    sorted_items = sorted_items[:topn]\n \n    score_vals = []\n    feature_vals = []\n    \n    # word index and corresponding tf-idf score\n    for idx, score in sorted_items:\n        \n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n \n    #create a tuples of feature,score\n    #results = zip(feature_vals,score_vals)\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    \n    return results\n#sort the tf-idf vectors by descending order of scores\nsorted_items=sort_coo(tf_idf_vector.tocoo())\n#extract only the top n; n here is 10\nkeywords=extract_topn_from_vector(feature_names,sorted_items,5)\n \n# now print the results\nprint(\"\\nAbstract:\")\nprint(doc)\nprint(\"\\nKeywords:\")\nfor k in keywords:\n    print(k,keywords[k])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Reference](https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34)"},{"metadata":{},"cell_type":"markdown","source":"# Approach 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n# LDA, tSNE\nfrom sklearn.manifold import TSNE\nfrom gensim.models.ldamodel import LdaModel\n# NLTK\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.corpus import stopwords\nimport re\n# Visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport matplotlib\n%matplotlib inline\nimport seaborn as sns\n# Bokeh\nfrom bokeh.io import output_notebook\nfrom bokeh.plotting import figure, show\nfrom bokeh.models import HoverTool, CustomJS, ColumnDataSource, Slider\nfrom bokeh.layouts import column\nfrom bokeh.palettes import all_palettes\noutput_notebook()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/nips-papers/papers.csv\")\nprint(df.paper_text[0][:500])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Processing"},{"metadata":{},"cell_type":"markdown","source":"### Initial cleaning\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Removing numerals:\ndf['paper_text_tokens'] = df.paper_text.map(lambda x: re.sub(r'\\d+', '', x))\n# Lower case:\ndf['paper_text_tokens'] = df.paper_text_tokens.map(lambda x: x.lower())\nprint(df['paper_text_tokens'][0][:500])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenize\nSpliting texts into separete words, also removing punctuanions and other stuff. After that procedure we should obtain texts as lists of words in lowercase:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndf['paper_text_tokens'] = df.paper_text_tokens.map(lambda x: RegexpTokenizer(r'\\w+').tokenize(x))\nprint(df['paper_text_tokens'][0][:25])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stemming\nStemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form ... The stem need not be identical to the morphological root of the word (see [[Wikipedia](https://en.wikipedia.org/wiki/Stemming)] for more details). We'll use SnowballStemmer from nltk package."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nsnowball = SnowballStemmer(\"english\")  \ndf['paper_text_tokens'] = df.paper_text_tokens.map(lambda x: [snowball.stem(token) for token in x])\nprint(df['paper_text_tokens'][0][:25])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stop words\nRemoving common English words like and, the, of and so on."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nstop_en = stopwords.words('english')\ndf['paper_text_tokens'] = df.paper_text_tokens.map(lambda x: [t for t in x if t not in stop_en]) \nprint(df['paper_text_tokens'][0][:25])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Final cleaning\nHere we'll remove all \"extremely short\" words (that have less than 2 characters):"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndf['paper_text_tokens'] = df.paper_text_tokens.map(lambda x: [t for t in x if len(t) > 1])\nprint(df['paper_text_tokens'][0][:25])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LDA\nFinally, let's use LDA ([Latent Dirichlet allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)) to extract topic structure from the corpus of texts."},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim import corpora, models\nnp.random.seed(2017)\ntexts = df['paper_text_tokens'].values\ndictionary = corpora.Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]\nldamodel = models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=8, passes=5, minimum_probability=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ldamodel.print_topics()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Refactoring results of LDA into numpy matrix (number_of_papers x number_of_topics)."},{"metadata":{"trusted":true},"cell_type":"code","source":"hm = np.array([[y for (x,y) in ldamodel[corpus[i]]] for i in range(len(corpus))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = TSNE(random_state=2017, perplexity=30, early_exaggeration=120)\nembedding = tsne.fit_transform(hm)\nembedding = pd.DataFrame(embedding, columns=['x','y'])\nembedding['hue'] = hm.argmax(axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ploting\nUsing Bokeh for scatter plot with interactions. Hover mouse over a dot to see the title of the respective paper:"},{"metadata":{"trusted":true},"cell_type":"code","source":"source = ColumnDataSource(\n        data=dict(\n            x = embedding.x,\n            y = embedding.y,\n            colors = [all_palettes['Set1'][8][i] for i in embedding.hue],\n            title = df.title,\n            year = df.year,\n            alpha = [0.9] * embedding.shape[0],\n            size = [7] * embedding.shape[0]\n        )\n    )\nhover_tsne = HoverTool(names=[\"df\"], tooltips=\"\"\"\n    <div style=\"margin: 10\">\n        <div style=\"margin: 0 auto; width:300px;\">\n            <span style=\"font-size: 12px; font-weight: bold;\">Title:</span>\n            <span style=\"font-size: 12px\">@title</span>\n            <span style=\"font-size: 12px; font-weight: bold;\">Year:</span>\n            <span style=\"font-size: 12px\">@year</span>\n        </div>\n    </div>\n    \"\"\")\ntools_tsne = [hover_tsne, 'pan', 'wheel_zoom', 'reset']\nplot_tsne = figure(plot_width=700, plot_height=700, tools=tools_tsne, title='Papers')\nplot_tsne.circle('x', 'y', size='size', fill_color='colors', \n                 alpha='alpha', line_alpha=0, line_width=0.01, source=source, name=\"df\")\n\ncallback = CustomJS(args=dict(source=source), code=\n    \"\"\"\n    var data = source.data;\n    var f = cb_obj.value\n    x = data['x']\n    y = data['y']\n    colors = data['colors']\n    alpha = data['alpha']\n    title = data['title']\n    year = data['year']\n    size = data['size']\n    for (i = 0; i < x.length; i++) {\n        if (year[i] <= f) {\n            alpha[i] = 0.9\n            size[i] = 7\n        } else {\n            alpha[i] = 0.05\n            size[i] = 4\n        }\n    }\n    source.change.emit();\n    \"\"\")\n\nslider = Slider(start=df.year.min(), end=df.year.max(), value=2016, step=1, title=\"Before year\")\nslider.js_on_change('value', callback)\n\nlayout = column(slider, plot_tsne)\nshow(layout)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}